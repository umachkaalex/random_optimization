Every day any business seeks ways for best optimization of it's processes.
- Someone needs to minimize empty space in sea containers to maximize amount of goods that could be shipped
because this will minimize costs and thus maximize profits.
- Airplane designers on one side have to minimize the size of aircraft to let it land at many airports as it possible
but on the other it's better to maximize its size to let it carry as more passengers as it's possible. 
- Candy makers every month have to optimize the plan of production for hundreds of SKUs according to current demand (with correction on upcoming seasonality) to maximize revenues but then it's desirable to minimize also costs - and to do this it's also would need to optimize production of certain SKUs, but this may not match the goals of optimization in purpose of maximization of revenues.

In the age of big data, productive computers and massive cloud computing, to solve such optimization problems became more convenient. But on the other hand - the bigger data you want to process the more complicated calculations you have to conduct. Let's take an example from real world - from the world of stock market.

More here:
https://nbviewer.jupyter.org/github/umachkaalex/stockmarket/blob/master/algo_for_medium.ipynb
